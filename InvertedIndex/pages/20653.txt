https://en.wikipedia.org/wiki/Computer_multitasking




Computer multitasking - Wikipedia



































Jump to content







Main menu





Main menu
move to sidebar
hide



		Navigation
	


Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us





		Contribute
	


HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages



















Search











Search






















Appearance
















Donate

Create account

Log in








Personal tools





Donate Create account Log in





		Pages for logged out editors learn more



ContributionsTalk




























Contents
move to sidebar
hide




(Top)





1
Multiprogramming








2
Cooperative multitasking








3
Preemptive multitasking








4
Real time








5
Multithreading








6
Memory protection








7
Memory swapping








8
Programming








9
See also








10
References


















Toggle the table of contents







Computer multitasking



41 languages




العربيةAzərbaycancaCatalàČeštinaDanskDeutschEestiEspañolEsperantoEuskaraفارسیFrançais한국어ՀայերենHrvatskiBahasa IndonesiaÍslenskaItalianoעבריתҚазақшаLombardMagyarമലയാളംNederlands日本語Norsk bokmålNorsk nynorskPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSlovenščinaکوردیSuomiSvenskaไทยУкраїнськаTiếng Việt中文

Edit links











ArticleTalk





English

















ReadEditView history







Tools





Tools
move to sidebar
hide



		Actions
	


ReadEditView history





		General
	


What links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code





		Print/export
	


Download as PDFPrintable version





		In other projects
	


Wikidata item





















Appearance
move to sidebar
hide










From Wikipedia, the free encyclopedia


Concurrent execution of multiple processes
Modern desktop operating systems are capable of handling large numbers of different processes at the same time. This screenshot shows Linux Mint running simultaneously Xfce desktop environment, Firefox, a calculator program, the built-in calendar, Vim, GIMP, and VLC media player.
Multitasking of Microsoft Windows 1.01 released in 1985, here shown running the MS-DOS Executive and Calculator programs
In computing, multitasking is the concurrent execution of multiple tasks (also known as processes) over a certain period of time.  New tasks can interrupt already started ones before they finish, instead of waiting for them to end. As a result, a computer executes segments of multiple tasks in an interleaved manner, while the tasks share common processing resources such as central processing units (CPUs) and main memory. Multitasking automatically interrupts the running program, saving its state (partial results, memory contents and computer register contents) and loading the saved state of another program and transferring control to it. This "context switch" may be initiated at fixed time intervals (pre-emptive multitasking), or the running program may be coded to signal to the supervisory software when it can be interrupted (cooperative multitasking).
Multitasking does not require parallel execution of multiple tasks at exactly the same time; instead, it allows more than one task to advance over a given period of time.[1]  Even on multiprocessor computers, multitasking allows many more tasks to be run than there are CPUs.
Multitasking is a common feature of computer operating systems since at least the 1960s.  It allows more efficient use of the computer hardware; when a program is waiting for some external event such as a user input or an input/output transfer with a peripheral to complete, the central processor can still be used with another program. In a time-sharing system, multiple human operators use the same processor as if it was dedicated to their use, while behind the scenes the computer is serving many users by multitasking their individual programs. In multiprogramming systems, a task runs until it must wait for an external event or until the operating system's scheduler forcibly swaps the running task out of the CPU. Real-time systems such as those designed to control industrial robots, require timely processing; a single processor might be shared between calculations of machine movement, communications, and user interface.[2]
Often multitasking operating systems include measures to change the priority of individual tasks, so that important jobs receive more processor time than those considered less significant. Depending on the operating system, a task might be as large as an entire application program, or might be made up of smaller threads that carry out portions of the overall program.
A processor intended for use with multitasking operating systems may include special hardware to securely support multiple tasks, such as memory protection, and protection rings that ensure the supervisory software cannot be damaged or subverted by user-mode program errors.
The term "multitasking" has become an international term, as the same word is used in many other languages such as German, Italian, Dutch, Romanian, Czech, Danish and Norwegian.


Multiprogramming[edit]
In the early days of computing, CPU time was expensive, and peripherals were very slow. When the computer ran a program that needed access to a peripheral, the central processing unit (CPU) would have to stop executing program instructions while the peripheral processed the data. This was usually very inefficient. Multiprogramming is a computing technique that enables multiple programs to be concurrently loaded and executed into a computer's memory, allowing the CPU to switch between them swiftly. This optimizes CPU utilization by keeping it engaged with the execution of tasks, particularly useful when one program is waiting for I/O operations to complete.
The Bull Gamma 60, initially designed in 1957 and first released in 1960, was the first computer designed with multiprogramming in mind. Its architecture featured a central memory and a Program Distributor feeding up to twenty-five autonomous processing units with code and data, and allowing concurrent operation of multiple clusters.
Another such computer was the LEO III, first released in 1961. During batch processing, several different programs were loaded in the computer memory, and the first one began to run. When the first program reached an instruction waiting for a peripheral, the context of this program was stored away, and the second program in memory was given a chance to run. The process continued until all programs finished running.[3]
Multiprogramming gives no guarantee that a program will run in a timely manner. Indeed, the first program may very well run for hours without needing access to a peripheral. As there were no users waiting at an interactive terminal, this was no problem: users handed in a deck of punched cards to an operator, and came back a few hours later for printed results. Multiprogramming greatly reduced wait times when multiple batches were being processed.[4][5]

Cooperative multitasking[edit]
Main article: Cooperative multitasking
Early multitasking systems used applications that voluntarily ceded time to one another. This approach, which was eventually supported by many computer operating systems, is known today as cooperative multitasking. Although it is now rarely used in larger systems except for specific applications such as CICS or the JES2 subsystem, cooperative multitasking was once the only scheduling scheme employed by Microsoft Windows and classic Mac OS to enable multiple applications to run simultaneously.  Cooperative multitasking is still used today on RISC OS systems.[6]
As a cooperatively multitasked system relies on each process regularly giving up time to other processes on the system, one poorly designed program can consume all of the CPU time for itself, either by performing extensive calculations or by busy waiting; both would cause the whole system to hang. In a server environment, this is a hazard that makes the entire environment unacceptably fragile.

Preemptive multitasking[edit]
Main article: Preemption (computing) § Preemptive multitasking
Kubuntu (KDE Plasma 5) four Virtual desktops running multiple programs at the same time
Preemptive multitasking allows the computer system to more reliably guarantee to each process a regular "slice" of operating time. It also allows the system to deal rapidly with important external events like incoming data, which might require the immediate attention of one or another process.  Operating systems were developed to take advantage of these hardware capabilities and run multiple processes preemptively. Preemptive multitasking was implemented in the PDP-6 Monitor and Multics in 1964, in OS/360 MFT in 1967, and in Unix in 1969, and was available in some operating systems for computers as small as DEC's PDP-8; it is a core feature of all Unix-like operating systems, such as Linux, Solaris and BSD with its derivatives,[7] as well as modern versions of Windows.
Possibly the earliest preemptive multitasking OS available to home users was Microware's OS-9, available for computers based on the Motorola 6809 such as the TRS-80 Color Computer 2,[8] with the operating system supplied by Tandy as an upgrade for disk-equipped systems.[9] Sinclair QDOS on the Sinclair QL followed in 1984, but it was not a big success. Commodore's Amiga was released the following year, offering a combination of multitasking and multimedia capabilities. Microsoft made preemptive multitasking a core feature of their flagship operating system in the early 1990s when developing Windows NT 3.1 and then Windows 95. In 1988 Apple offered A/UX as a UNIX System V-based alternative to the Classic Mac OS. In 2001 Apple switched to the NeXTSTEP-influenced Mac OS X.
A similar model is used in Windows 9x and the Windows NT family, where native 32-bit applications are multitasked preemptively.[10] 64-bit editions of Windows, both for the x86-64 and Itanium architectures, no longer support legacy 16-bit applications, and thus provide preemptive multitasking for all supported applications.

Real time[edit]
Another reason for multitasking was in the design of real-time computing systems, where there are a number of possibly unrelated external activities needed to be controlled by a single processor system. In such systems a hierarchical interrupt system is coupled with process prioritization to ensure that key activities were given a greater share of available process time.[11]

Multithreading[edit]
Threads were born from the idea that the most efficient way for cooperating processes to exchange data would be to share their entire memory space. Thus, threads are effectively processes that run in the same memory context and share other resources with their parent processes, such as open files. Threads are described as lightweight processes because switching between threads does not involve changing the memory context.[12][13][14]
While threads are scheduled preemptively, some operating systems provide a variant to threads, named fibers, that are scheduled cooperatively. On operating systems that do not provide fibers, an application may implement its own fibers using repeated calls to worker functions. Fibers are even more lightweight than threads, and somewhat easier to program with, although they tend to lose some or all of the benefits of threads on machines with multiple processors.[15]
Some systems directly support multithreading in hardware.

Memory protection[edit]
Main article: Memory protection
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2025) (Learn how and when to remove this message)
Essential to any multitasking system is to safely and effectively share access to system resources. Access to memory must be strictly managed to ensure that no process can inadvertently or deliberately read or write to memory locations outside the process's address space. This is done for the purpose of general system stability and data integrity, as well as data security.
In general, memory access management is a responsibility of the operating system kernel, in combination with hardware mechanisms that provide supporting functionalities, such as a memory management unit (MMU). If a process attempts to access a memory location outside its memory space, the MMU denies the request and signals the kernel to take appropriate actions; this usually results in forcibly terminating the offending process. Depending on the software and kernel design and the specific error in question, the user may receive an access violation error message such as "segmentation fault".
In a well designed and correctly implemented multitasking system, a given process can never directly access memory that belongs to another process. An exception to this rule is in the case of shared memory; for example, in the System V inter-process communication mechanism the kernel allocates memory to be mutually shared by multiple processes. Such features are often used by database management software such as PostgreSQL.
Inadequate memory protection mechanisms, either due to flaws in their design or poor implementations, allow for security vulnerabilities that may be potentially exploited by malicious software.

Memory swapping[edit]
Use of a swap file or swap partition is a way for the operating system to provide more memory than is physically available by keeping portions of the primary memory in secondary storage. While multitasking and memory swapping are two completely unrelated techniques, they are very often used together, as swapping memory allows more tasks to be loaded at the same time.  Typically, a multitasking system allows another process to run when the running process hits a point where it has to wait for some portion of memory to be reloaded from secondary storage.[16]

Programming[edit]
Over the years, multitasking systems have been refined. Modern operating systems generally include detailed mechanisms for prioritizing processes, while symmetric multiprocessing has introduced new complexities and capabilities.[17]

See also[edit]
Process state
Task switching
References[edit]


^ "Concurrency vs Parallelism, Concurrent Programming vs Parallel Programming". Oracle. Archived from the original on April 7, 2016. Retrieved March 23, 2016.

^ Anthony Ralston, Edwin D. Reilly (ed),Encyclopedia of Computer Science Third Edition, Van Nostrand Reinhold, 1993, ISBN 0-442-27679-6, articles "Multitasking" and "Multiprogramming"

^ MASTER PROGRAME AND PROGRAMME TRIALS SYSTEM PART 1 MASTER PROGRAMME SPECIFICATION. February 1965. section 6 "PRIORITY CONTROL ROUTINES".

^ Lithmee (2019-05-20). "What is the Difference Between Batch Processing and Multiprogramming". Pediaa.Com. Retrieved 2020-04-14.

^ "Evolution of Operating System". 2017-09-29. Retrieved 2020-04-14.

^ 
"Preemptive multitasking". riscos.info. 2009-11-03. Retrieved 2014-07-27.

^ "UNIX, Part One". The Digital Research Initiative. ibiblio.org. 2002-01-30. Retrieved 2014-01-09.

^ Downard, Dan (September 1983). "Dynamic Uno". The Rainbow. pp. 236–240. Retrieved 9 May 2024.

^ 1984 TRS-80 Catalog. Tandy Corporation. 1984. pp. 53–54. Retrieved 14 May 2024.

^ Joseph Moran (June 2006). "Windows 2000 &16-Bit Applications". Smart Computing. Vol. 16, no. 6. pp. 32–33. Archived from the original on January 25, 2009.

^ Liu, C. L.; Layland, James W. (1973-01-01). "Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment". Journal of the ACM. 20 (1): 46–61. doi:10.1145/321738.321743. ISSN 0004-5411. S2CID 59896693.

^ Eduardo Ciliendo; Takechika Kunimasa (April 25, 2008). "Linux Performance and Tuning Guidelines" (PDF). redbooks.ibm.com. IBM. p. 4. Archived from the original (PDF) on February 26, 2015. Retrieved March 1, 2015.

^ "Context Switch Definition". linfo.org. May 28, 2006. Archived from the original on February 18, 2010. Retrieved February 26, 2015.

^ "What are threads (user/kernel)?". tldp.org. September 8, 1997. Retrieved February 26, 2015.

^ Multitasking different methods

Accessed on February 19, 2019

^ "What is a swap file?". kb.iu.edu. Retrieved 2018-03-26.

^ "Operating Systems Architecture". cis2.oc.ctc.edu. Retrieved 2018-03-17.


vteOperating systemsGeneral
Comparison
Forensic engineering
History
List
Timeline
Usage share
User features comparison
Variants
Disk operating system
Distributed operating system
Embedded operating system
Hobbyist operating system
Just enough operating system
Mobile operating system
Network operating system
Object-oriented operating system
Real-time operating system
Supercomputer operating system
KernelArchitectures
Exokernel
Hybrid
Microkernel
Monolithic
Multikernel
vkernel
Rump kernel
Unikernel
Components
Device driver
Loadable kernel module
User space and kernel space
Process managementConcepts
Computer multitasking (Cooperative, Preemptive)
Context switch
Interrupt
IPC
Process
Process control block
Real-time
Thread
Time-sharing
Schedulingalgorithms
Fixed-priority preemptive
Multilevel feedback queue
Round-robin
Shortest job next
Memory management,resource protection
Bus error
General protection fault
Memory paging
Memory protection
Protection ring
Segmentation fault
Virtual memory
Storage access,file systems
Boot loader
Defragmentation
Device file
File attribute
Inode
Journal
Partition
Virtual file system
Virtual tape library
Supporting concepts
API
Computer network
HAL
Live CD
Live USB
Shell
CLI
User interface
PXE

vteParallel computingGeneral
Distributed computing
Parallel computing
Massively parallel
Cloud computing
High-performance computing
Multiprocessing
Manycore processor
GPGPU
Computer network
Systolic array
Levels
Bit
Instruction
Thread
Task
Data
Memory
Loop
Pipeline
Multithreading
Temporal
Simultaneous (SMT)
Simultaneous and heterogenous
Speculative (SpMT)
Preemptive
Cooperative
Clustered multi-thread (CMT)
Hardware scout
Theory
PRAM model
PEM model
Analysis of parallel algorithms
Amdahl's law
Gustafson's law
Cost efficiency
Karp–Flatt metric
Slowdown
Speedup
Elements
Process
Thread
Fiber
Instruction window
Array
Coordination
Multiprocessing
Memory coherence
Cache coherence
Cache invalidation
Barrier
Synchronization
Application checkpointing
Programming
Stream processing
Dataflow programming
Models
Implicit parallelism
Explicit parallelism
Concurrency
Non-blocking algorithm
Hardware
Flynn's taxonomy
SISD
SIMD
Array processing (SIMT)
Pipelined processing
Associative processing
MISD
MIMD
Dataflow architecture
Pipelined processor
Superscalar processor
Vector processor
Multiprocessor
symmetric
asymmetric
Memory
shared
distributed
distributed shared
UMA
NUMA
COMA
Massively parallel computer
Computer cluster
Beowulf cluster
Grid computer
Hardware acceleration
APIs
Ateji PX
Boost
Chapel
HPX
Charm++
Cilk
Coarray Fortran
CUDA
Dryad
C++ AMP
Global Arrays
GPUOpen
MPI
OpenMP
OpenCL
OpenHMPP
OpenACC
Parallel Extensions
PVM
pthreads
RaftLib
ROCm
UPC
TBB
ZPL
Problems
Automatic parallelization
Deadlock
Deterministic algorithm
Embarrassingly parallel
Parallel slowdown
Race condition
Software lockout
Scalability
Starvation

 Category: Parallel computing

Authority control databases: National GermanyCzech RepublicIsrael




Retrieved from "https://en.wikipedia.org/w/index.php?title=Computer_multitasking&oldid=1282758556"
Categories: Concurrent computingOperating system technologyHidden categories: Articles with short descriptionShort description is different from WikidataArticles needing additional references from March 2025All articles needing additional references






 This page was last edited on 28 March 2025, at 12:36 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike 4.0 License;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Code of Conduct
Developers
Statistics
Cookie statement
Mobile view














Search













Search









Toggle the table of contents







Computer multitasking




























41 languages


Add topic
















