https://en.wikipedia.org/wiki/Single_instruction,_multiple_data




Single instruction, multiple data - Wikipedia




































Jump to content







Main menu





Main menu
move to sidebar
hide



		Navigation
	


Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us





		Contribute
	


HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages



















Search











Search






















Appearance
















Donate

Create account

Log in








Personal tools





Donate Create account Log in





		Pages for logged out editors learn more



ContributionsTalk




























Contents
move to sidebar
hide




(Top)





1
History








2
Advantages








3
Disadvantages








4
Chronology








5
Hardware








6
Software




Toggle Software subsection





6.1
Programmer interface








6.2
SIMD multi-versioning










7
SIMD on the web








8
Commercial applications








9
See also








10
References








11
External links


















Toggle the table of contents







Single instruction, multiple data



25 languages




العربيةCatalàČeštinaDeutschEestiEspañolفارسیFrançaisGalego한국어Bahasa IndonesiaItalianoMagyarNederlands日本語Norsk bokmålPolskiPortuguêsRomânăРусскийSlovenčinaСрпски / srpskiSuomiУкраїнська中文

Edit links











ArticleTalk





English

















ReadEditView history







Tools





Tools
move to sidebar
hide



		Actions
	


ReadEditView history





		General
	


What links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code





		Print/export
	


Download as PDFPrintable version





		In other projects
	


Wikidata item





















Appearance
move to sidebar
hide










From Wikipedia, the free encyclopedia


Type of parallel processing
"SIMD" redirects here. For the cryptographic hash function, see SIMD (hash function). For the Scottish statistical tool, see Scottish index of multiple deprivation.
This article's factual accuracy may be compromised due to out-of-date information. Please help update this article to reflect recent events or newly available information. (March 2017)
Flynn's taxonomy
Single data stream
SISD
MISD

Multiple data streams
SIMD
MIMD

SIMD subcategories[1]
Array processing (SIMT)
Pipelined processing (packed SIMD)
Associative processing (predicated/masked SIMD)

See also
SPMD
MPMD

Single instruction, multiple data
Single instruction, multiple data (SIMD) is a type of parallel processing in Flynn's taxonomy. SIMD describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously. SIMD can be internal (part of the hardware design) and it can be directly accessible through an instruction set architecture (ISA), but it should not be confused with an ISA. 
Such machines exploit data level parallelism, but not concurrency: there are simultaneous (parallel) computations, but each unit performs exactly the same instruction at any given moment (just with different data). A simple example is to add many pairs of numbers together, all of the SIMD units are performing an addition, but each one has different pairs of values to add. SIMD is particularly applicable to common tasks such as adjusting the contrast in a digital image or adjusting the volume of digital audio. Most modern CPU designs include SIMD instructions to improve the performance of multimedia use. In recent CPUs, SIMD units are tightly coupled with cache hierarchies and prefetch mechanisms, which minimize latency during large block operations. For instance, AVX-512-enabled processors can prefetch entire cache lines and apply fused multiply-add operations (FMA) in a single SIMD cycle.
SIMD has three different subcategories in Flynn's 1972 Taxonomy, one of which is SIMT. SIMT should not be confused with software threads or hardware threads, both of which are task time-sharing (time-slicing). SIMT is true simultaneous parallel hardware-level execution. A key distinction in SIMT is the presence of control flow mechanisms like warps (NVIDIA terminology) or wavefronts (AMD terminology). These allow divergence and convergence of threads, even under shared instruction streams, thereby offering slightly more flexibility than classical SIMD.
Each hardware element (PU) working on individual data item sometimes also referred as SIMD lane or channel. Modern graphics processing units (GPUs) are often wide SIMD (typically >16 data lanes or channel) implementations.[citation needed] Some newer GPUs go beyond simple SIMD and integrate mixed-precision SIMD pipelines, which allow concurrent execution of 8-bit, 16-bit, and 32-bit operations in different lanes. This is critical for applications like AI inference, where mixed precision boosts throughput.
Additionally, SIMD can exist in both fixed and scalable vector forms. Fixed-width SIMD units operate on a constant number of data points per instruction, while scalable designs, like RISC-V Vector or ARM's SVE, allow the number of data elements to vary depending on the hardware implementation. This improves forward compatibility across generations of processors.


History[edit]
The first use of SIMD instructions was in the ILLIAC IV, which was completed in 1972. This included 64 (of an original design of 256) processors that had local memory to hold different values while performing the same instruction. Separate hardware quickly send out the values to be processed and gathered up the results.
SIMD was the basis for vector supercomputers of the early 1970s such as the CDC Star-100 and the Texas Instruments ASC, which could operate on a "vector" of data with a single instruction. Vector processing was especially popularized by Cray in the 1970s and 1980s. Vector processing architectures are now considered separate from SIMD computers: Duncan's Taxonomy includes them whereas Flynn's Taxonomy does not, due to Flynn's work (1966, 1972) pre-dating the Cray-1 (1977).
The first era of modern SIMD computers was characterized by massively parallel processing-style supercomputers such as the Thinking Machines CM-1 and CM-2. These computers had many limited-functionality processors that would work in parallel. For example, each of 65,536 single-bit processors in a Thinking Machines CM-2 would execute the same instruction at the same time, allowing, for instance, to logically combine 65,536 pairs of bits at a time, using a hypercube-connected network or processor-dedicated RAM to find its operands. Supercomputing moved away from the SIMD approach when inexpensive scalar MIMD approaches based on commodity processors such as the Intel i860 XP became more powerful, and interest in SIMD waned.[2]
The current era of SIMD processors grew out of the desktop-computer market rather than the supercomputer market. As desktop processors became powerful enough to support real-time gaming and audio/video processing during the 1990s, demand grew for this particular type of computing power, and microprocessor vendors turned to SIMD to meet the demand.[3] This resurgence also coincided with the rise of DirectX and OpenGL shader models, which heavily leveraged SIMD under the hood. The graphics APIs encouraged programmers to adopt data-parallel programming styles, indirectly accelerating SIMD adoption in desktop software. Hewlett-Packard introduced MAX instructions into PA-RISC 1.1 desktops in 1994 to accelerate MPEG decoding.[4] Sun Microsystems introduced SIMD integer instructions in its "VIS" instruction set extensions in 1995, in its UltraSPARC I microprocessor. MIPS followed suit with their similar MDMX system.
The first widely deployed desktop SIMD was with Intel's MMX extensions to the x86 architecture in 1996. This sparked the introduction of the much more powerful AltiVec system in the Motorola PowerPC and IBM's POWER systems. Intel responded in 1999 by introducing the all-new SSE system. Since then, there have been several extensions to the SIMD instruction sets for both architectures. Advanced vector extensions AVX, AVX2 and AVX-512 are developed by Intel. AMD supports AVX, AVX2, and AVX-512 in their current products.[5]
All of these developments have been oriented toward support for real-time graphics, and are therefore oriented toward processing in two, three, or four dimensions, usually with vector lengths of between two and sixteen words, depending on data type and architecture. When new SIMD architectures need to be distinguished from older ones, the newer architectures are then considered "short-vector" architectures, as earlier SIMD and vector supercomputers had vector lengths from 64 to 64,000. A modern supercomputer is almost always a cluster of MIMD computers, each of which implements (short-vector) SIMD instructions.

Advantages[edit]
An application that may take advantage of SIMD is one where the same value is being added to (or subtracted from) a large number of data points, a common operation in many multimedia applications. One example would be changing the brightness of an image. Each pixel of an image consists of three values for the brightness of the red (R), green (G) and blue (B) portions of the color. To change the brightness, the R, G and B values are read from memory, a value is added to (or subtracted from) them, and the resulting values are written back out to memory.  Audio DSPs would likewise, for volume control, multiply both Left and Right channels simultaneously.
With a SIMD processor there are two improvements to this process. For one the data is understood to be in blocks, and a number of values can be loaded all at once. Instead of a series of instructions saying "retrieve this pixel, now retrieve the next pixel", a SIMD processor will have a single instruction that effectively says "retrieve n pixels" (where n is a number that varies from design to design). For a variety of reasons, this can take much less time than retrieving each pixel individually, as with a traditional CPU design. Moreover, SIMD instructions can exploit data reuse, where the same operand is used across multiple calculations, via broadcasting features. For example, multiplying several pixels by a constant scalar value can be done more efficiently by loading the scalar once and broadcasting it across a SIMD register.
Another advantage is that the instruction operates on all loaded data in a single operation. In other words, if the SIMD system works by loading up eight data points at once, the add operation being applied to the data will happen to all eight values at the same time. This parallelism is separate from the parallelism provided by a superscalar processor; the eight values are processed in parallel even on a non-superscalar processor, and a superscalar processor may be able to perform multiple SIMD operations in parallel.

Disadvantages[edit]
Not all algorithms can be vectorized easily. For example, a flow-control-heavy task like code parsing may not easily benefit from SIMD; however, it is theoretically possible to vectorize comparisons and "batch flow" to target maximal cache optimality, though this technique will require more intermediate state. Note: Batch-pipeline systems (example: GPUs or software rasterization pipelines) are most advantageous for cache control when implemented with SIMD intrinsics, but they are not exclusive to SIMD features. Further complexity may be apparent to avoid dependence within series such as code strings; while independence is required for vectorization.[clarification needed] Additionally, divergent control flow—where different data lanes would follow different execution paths—can lead to underutilization of SIMD hardware. To handle such divergence, techniques like masking and predication are often employed, but they introduce performance overhead and complexity.
Large register files which increases power consumption and required chip area.
Currently, implementing an algorithm with SIMD instructions usually requires human labor; most compilers do not generate SIMD instructions from a typical C program, for instance. Automatic vectorization in compilers is an active area of computer science research. (Compare vector processing.)
Programming with particular SIMD instruction sets can involve numerous low-level challenges.
SIMD may have restrictions on data alignment; programmers familiar with one particular architecture may not expect this. Worse: the alignment may change from one revision or "compatible" processor to another.
Gathering data into SIMD registers and scattering it to the correct destination locations is tricky (sometimes requiring permute operations) and can be inefficient.
Specific instructions like rotations or three-operand addition are not available in some SIMD instruction sets.
Instruction sets are architecture-specific: some processors lack SIMD instructions entirely, so programmers must provide non-vectorized implementations (or different vectorized implementations) for them.
Different architectures provide different register sizes (e.g. 64, 128, 256 and 512 bits) and instruction sets, meaning that programmers must provide multiple implementations of vectorized code to operate optimally on any given CPU. In addition, the possible set of SIMD instructions grows with each new register size. Unfortunately, for legacy support reasons, the older versions cannot be retired.
The early MMX instruction set shared a register file with the floating-point stack, which caused inefficiencies when mixing floating-point and MMX code. However, SSE2 corrects this.
To remedy problems 1 and 5, RISC-V's vector extension uses an alternative approach: instead of exposing the sub-register-level details to the programmer, the instruction set abstracts them out as a few "vector registers" that use the same interfaces across all CPUs with this instruction set. The hardware handles all alignment issues and "strip-mining" of loops. Machines with different vector sizes would be able to run the same code. LLVM calls this vector type "vscale".[citation needed]
An order of magnitude increase in code size is not uncommon, when compared to equivalent scalar or equivalent vector code, and an order of magnitude or greater effectiveness (work done per instruction) is achievable with Vector ISAs.[6]
ARM's Scalable Vector Extension takes another approach, known in Flynn's Taxonomy as "Associative Processing", more commonly known today as "Predicated" (masked) SIMD. This approach is not as compact as Vector processing but is still far better than non-predicated SIMD. Detailed comparative examples are given in the Vector processing page. In addition, all versions of the ARM architecture have offered Load and Store multiple instructions, to Load or Store a block of data from a continuous block of memory, into a range or non-continuous set of registers.[7]

Chronology[edit]

Examples of SIMD supercomputers (not including vector processors)


Year
Example


1974
ILLIAC IV


1974
ICL Distributed Array Processor (DAP)


1976
Burroughs Scientific Processor


1981
Geometric-Arithmetic Parallel Processor from Martin Marietta (continued at Lockheed Martin, then at Teranex and Silicon Optix)


1983-1991
Massively Parallel Processor (MPP), from NASA/Goddard Space Flight Center


1985
Connection Machine, models 1 and 2 (CM-1 and CM-2), from Thinking Machines Corporation


1987-1996
MasPar MP-1 and MP-2


1991
Zephyr DC from Wavetracer


2001
Xplor from Pyxsys, Inc.

Hardware[edit]
Small-scale (64 or 128 bits) SIMD became popular on general-purpose CPUs in the early 1990s and continued through 1997 and later with Motion Video Instructions (MVI) for Alpha. SIMD instructions can be found, to one degree or another, on most CPUs, including IBM's AltiVec and SPE for PowerPC, HP's PA-RISC Multimedia Acceleration eXtensions (MAX), Intel's MMX and iwMMXt, SSE, SSE2, SSE3 SSSE3 and SSE4.x, AMD's 3DNow!, ARC's ARC Video subsystem, SPARC's VIS and VIS2, Sun's MAJC, ARM's Neon technology, MIPS' MDMX (MaDMaX) and MIPS-3D. The IBM, Sony, Toshiba co-developed Cell Processor's SPU's instruction set is heavily SIMD based. Philips, now NXP, developed several SIMD processors named Xetal. The Xetal has 320 16-bit processor elements especially designed for vision tasks. Apple's M1 and M2 chips also incorporate SIMD units deeply integrated with their GPU and Neural Engine, using Apple-designed SIMD pipelines optimized for image filtering, convolution, and matrix multiplication. This unified memory architecture helps SIMD instructions operate on shared memory pools more efficiently.
Intel's AVX-512 SIMD instructions process 512 bits of data at once.

Software[edit]
 The ordinary tripling of four 8-bit numbers. The CPU loads one 8-bit number into R1, multiplies it with R2, and then saves the answer from R3 back to RAM. This process is repeated for each number.
 The SIMD tripling of four 8-bit numbers. The CPU loads 4 numbers at once, multiplies them all in one SIMD-multiplication, and saves them all at once back to RAM. In theory, the speed can be multiplied by 4.
SIMD instructions are widely used to process 3D graphics, although modern graphics cards with embedded SIMD have largely taken over this task from the CPU. Some systems also include permute functions that re-pack elements inside vectors, making them particularly useful for data processing and compression. They are also used in cryptography.[8][9][10] The trend of general-purpose computing on GPUs (GPGPU) may lead to wider use of SIMD in the future. Recent compilers such as LLVM, GCC, and Intel's ICC offer aggressive auto-vectorization options. Developers can often enable these with flags like -O3 or -ftree-vectorize, which guide the compiler to restructure loops for SIMD compatibility.
Adoption of SIMD systems in personal computer software was at first slow, due to a number of problems. One was that many of the early SIMD instruction sets tended to slow overall performance of the system due to the re-use of existing floating point registers. Other systems, like MMX and 3DNow!, offered support for data types that were not interesting to a wide audience and had expensive context switching instructions to switch between using the FPU and MMX registers. Compilers also often lacked support, requiring programmers to resort to assembly language coding.
SIMD on x86 had a slow start. The introduction of 3DNow! by AMD and SSE by Intel confused matters somewhat, but today the system seems to have settled down (after AMD adopted SSE) and newer compilers should result in more SIMD-enabled software. Intel and AMD now both provide optimized math libraries that use SIMD instructions, and open source alternatives like libSIMD, SIMDx86 and SLEEF have started to appear (see also libm).[11]
Apple Computer had somewhat more success, even though they entered the SIMD market later than the rest. AltiVec offered a rich system and can be programmed using increasingly sophisticated compilers from Motorola, IBM and GNU, therefore assembly language programming is rarely needed. Additionally, many of the systems that would benefit from SIMD were supplied by Apple itself, for example iTunes and QuickTime. However, in 2006, Apple computers moved to Intel x86 processors. Apple's APIs and development tools (XCode) were modified to support SSE2 and SSE3 as well as AltiVec. Apple was the dominant purchaser of PowerPC chips from IBM and Freescale Semiconductor. Even though Apple has stopped using PowerPC processors in their products, further development of AltiVec is continued in several PowerPC and Power ISA designs from Freescale and IBM.
SIMD within a register, or SWAR, is a range of techniques and tricks used for performing SIMD in general-purpose registers on hardware that does not provide any direct support for SIMD instructions. This can be used to exploit parallelism in certain algorithms even on hardware that does not support SIMD directly.

Programmer interface[edit]
It is common for publishers of the SIMD instruction sets to make their own C/C++ language extensions with intrinsic functions or special datatypes (with operator overloading) guaranteeing the generation of vector code. Intel, AltiVec, and ARM NEON provide extensions widely adopted by the compilers targeting their CPUs. (More complex operations are the task of vector math libraries.)
The GNU C Compiler takes the extensions a step further by abstracting them into a universal interface that can be used on any platform by providing a way of defining SIMD datatypes.[12] The LLVM Clang compiler also implements the feature, with an analogous interface defined in the IR.[13] Rust's packed_simd crate (and the experimental std::simd) uses this interface, and so does Swift 2.0+.
C++ has an experimental interface std::experimental::simd that works similarly to the GCC extension. LLVM's libcxx seems to implement it.[citation needed] For GCC and libstdc++, a wrapper library that builds on top of the GCC extension is available.[14]
Microsoft added SIMD to .NET in RyuJIT.[15] The System.Numerics.Vector package, available on NuGet, implements SIMD datatypes.[16] Java also has a new proposed API for SIMD instructions  available in OpenJDK 17 in an incubator module.[17] It also has a safe fallback mechanism on unsupported CPUs to simple loops.
Instead of providing an SIMD datatype, compilers can also be hinted to auto-vectorize some loops, potentially taking some assertions about the lack of data dependency. This is not as flexible as manipulating SIMD variables directly, but is easier to use. OpenMP 4.0+ has a #pragma omp simd hint.[18] This OpenMP interface has replaced a wide set of nonstandard extensions, including Cilk's #pragma simd,[19] GCC's #pragma GCC ivdep, and many more.[20]

SIMD multi-versioning[edit]
Consumer software is typically expected to work on a range of CPUs covering multiple generations, which could limit the programmer's ability to use new SIMD instructions to improve the computational performance of a program. The solution is to include multiple versions of the same code that uses either older or newer SIMD technologies, and pick one that best fits the user's CPU at run-time (dynamic dispatch). There are two main camps of solutions:

Function multi-versioning (FMV): a subroutine in the program or a library is duplicated and compiled for many instruction set extensions, and the program decides which one to use at run-time.
Library multi-versioning (LMV): the entire programming library is duplicated for many instruction set extensions, and the operating system or the program decides which one to load at run-time.
FMV, manually coded in assembly language, is quite commonly used in a number of performance-critical libraries such as glibc and libjpeg-turbo. Intel C++ Compiler, GNU Compiler Collection since GCC 6, and Clang since clang 7 allow for a simplified approach, with the compiler taking care of function duplication and selection. GCC and clang requires explicit target_clones labels in the code to "clone" functions,[21] while ICC does so automatically (under the command-line option /Qax). The Rust programming language also supports FMV. The setup is similar to GCC and Clang in that the code defines what instruction sets to compile for, but cloning is manually done via inlining.[22]
As using FMV requires code modification on GCC and Clang, vendors more commonly use library multi-versioning: this is easier to achieve as only compiler switches need to be changed. Glibc supports LMV and this functionality is adopted by the Intel-backed Clear Linux project.[23]

SIMD on the web[edit]
In 2013 John McCutchan announced that he had created a high-performance interface to SIMD instruction sets for the Dart programming language, bringing the benefits of SIMD to web programs for the first time. The interface consists of two types:[24]

Float32x4, 4 single precision floating point values.
Int32x4, 4 32-bit integer values.
Instances of these types are immutable and in optimized code are mapped directly to SIMD registers. Operations expressed in Dart typically are compiled into a single instruction without any overhead. This is similar to C and C++ intrinsics. Benchmarks for 4×4 matrix multiplication, 3D vertex transformation, and Mandelbrot set visualization show near 400% speedup compared to scalar code written in Dart.
McCutchan's work on Dart, now called SIMD.js, has been adopted by ECMAScript and Intel announced at IDF 2013 that they are implementing McCutchan's specification for both V8 and SpiderMonkey.[25] However, by 2017, SIMD.js has been taken out of the ECMAScript standard queue in favor of pursuing a similar interface in WebAssembly.[26] As of August 2020, the WebAssembly interface remains unfinished, but its portable 128-bit SIMD feature has already seen some use in many engines.[citation needed]
Emscripten, Mozilla's C/C++-to-JavaScript compiler, with extensions can enable compilation of C++ programs that make use of SIMD intrinsics or GCC-style vector code to the SIMD API of JavaScript, resulting in equivalent speedups compared to scalar code.[27] It also supports (and now prefers) the WebAssembly 128-bit SIMD proposal.[28]

Commercial applications[edit]
It has generally proven difficult to find sustainable commercial applications for SIMD-only processors.
One that has had some measure of success is the GAPP, which was developed by Lockheed Martin and taken to the commercial sector by their spin-off Teranex. The GAPP's recent incarnations have become a powerful tool in real-time video processing applications like conversion between various video standards and frame rates (NTSC to/from PAL, NTSC to/from HDTV formats, etc.), deinterlacing, image noise reduction, adaptive video compression, and image enhancement.
A more ubiquitous application for SIMD is found in video games: nearly every modern video game console since 1998 has incorporated a SIMD processor somewhere in its architecture. The PlayStation 2 was unusual in that one of its vector-float units could function as an autonomous DSP executing its own instruction stream, or as a coprocessor driven by ordinary CPU instructions. 3D graphics applications tend to lend themselves well to SIMD processing as they rely heavily on operations with 4-dimensional vectors. Microsoft's Direct3D 9.0 now chooses at runtime processor-specific implementations of its own math operations, including the use of SIMD-capable instructions.
A later processor that used vector processing is the Cell Processor used in the Playstation 3, which was developed by IBM in cooperation with Toshiba and Sony. It uses a number of SIMD processors (a NUMA architecture, each with independent local store and controlled by a general purpose CPU) and is geared towards the huge datasets required by 3D and video processing applications. It differs from traditional ISAs by being SIMD from the ground up with no separate scalar registers.
Ziilabs produced an SIMD type processor for use on mobile devices, such as media players and mobile phones.[29]
Larger scale commercial SIMD processors are available from ClearSpeed Technology, Ltd. and Stream Processors, Inc. ClearSpeed's CSX600 (2004) has 96 cores each with two double-precision floating point units while the CSX700 (2008) has 192. Stream Processors is headed by computer architect Bill Dally. Their Storm-1 processor (2007) contains 80 SIMD cores controlled by a MIPS CPU.

See also[edit]
Streaming SIMD Extensions, MMX, SSE2, SSE3, Advanced Vector Extensions, AVX-512
Instruction set architecture
Flynn's taxonomy
SIMD within a register (SWAR)
Single Program, Multiple Data (SPMD)
OpenCL
References[edit]


^ Flynn, Michael J. (September 1972). "Some Computer Organizations and Their Effectiveness" (PDF). IEEE Transactions on Computers. C-21 (9): 948–960. doi:10.1109/TC.1972.5009071.

^ "MIMD1 - XP/S, CM-5" (PDF).

^ Conte, G.; Tommesani, S.; Zanichelli, F. (2000). "The long and winding road to high-performance image processing with MMX/SSE". Proc. Fifth IEEE Int'l Workshop on Computer Architectures for Machine Perception. doi:10.1109/CAMP.2000.875989. hdl:11381/2297671. S2CID 13180531.

^ Lee, R.B. (1995). "Realtime MPEG video via software decompression on a PA-RISC processor". digest of papers Compcon '95. Technologies for the Information Superhighway. pp. 186–192. doi:10.1109/CMPCON.1995.512384. ISBN 0-8186-7029-0. S2CID 2262046.

^ "AMD Zen 4 AVX-512 Performance Analysis On The Ryzen 9 7950X Review". www.phoronix.com. Retrieved 2023-07-13.

^ Patterson, David; Waterman, Andrew (18 September 2017). "SIMD Instructions Considered Harmful". SIGARCH.

^ "ARM LDR/STR, LDM/STM instructions - Programmer All". programmerall.com. Retrieved 2025-04-19.

^ RE: SSE2 speed, showing how SSE2 is used to implement SHA hash algorithms

^ Salsa20 speed; Salsa20 software, showing a stream cipher implemented using SSE2

^ Subject: up to 1.4x RSA throughput using SSE2, showing RSA implemented using a non-SIMD SSE2 integer multiply instruction.

^ "SIMD library math functions". Stack Overflow. Retrieved 16 January 2020.

^ "Vector Extensions". Using the GNU Compiler Collection (GCC). Retrieved 16 January 2020.

^ "Clang Language Extensions". Clang 11 documentation. Retrieved 16 January 2020.

^ "VcDevel/std-simd". VcDevel. 6 August 2020.

^ "RyuJIT: The next-generation JIT compiler for .NET". 30 September 2013.

^ "The JIT finally proposed. JIT and SIMD are getting married". 7 April 2014.

^ "JEP 338: Vector API".

^ "SIMD Directives". www.openmp.org.

^ "Tutorial pragma simd". CilkPlus. 18 July 2012. Archived from the original on 4 December 2020. Retrieved 9 August 2020.

^ Kruse, Michael. "OMP5.1: Loop Transformations" (PDF).

^ "Function multi-versioning in GCC 6". lwn.net.

^ "2045-target-feature". The Rust RFC Book.

^ "Transparent use of library packages optimized for Intel® architecture". Clear Linux* Project. Retrieved 8 September 2019.

^ John McCutchan. "Bringing SIMD to the web via Dart" (PDF). Archived from the original (PDF) on 2013-12-03.

^ "SIMD in JavaScript". 01.org. 8 May 2014.

^ "tc39/ecmascript_simd: SIMD numeric type for EcmaScript". GitHub. Ecma TC39. 22 August 2019. Retrieved 8 September 2019.

^ Jensen, Peter; Jibaja, Ivan; Hu, Ningxin; Gohman, Dan; McCutchan, John (2015). "SIMD in JavaScript via C++ and Emscripten" (PDF).

^ "Porting SIMD code targeting WebAssembly". Emscripten 1.40.1 documentation.

^ "ZiiLABS ZMS-05 ARM 9 Media Processor". ZiiLabs. Archived from the original on 2011-07-18. Retrieved 2010-05-24.


External links[edit]
SIMD architectures (2000)
Cracking Open The Pentium 3 (1999)
Short Vector Extensions in Commercial Microprocessor
Article about Optimizing the Rendering Pipeline of Animated Models Using the Intel Streaming SIMD Extensions
"Yeppp!": cross-platform, open-source SIMD library from Georgia Tech
Introduction to Parallel Computing from LLNL Lawrence Livermore National Laboratory Archived 2013-06-10 at the Wayback Machine
simde on GitHub: A portable implementation of platform-specific intrinsics for other platforms (e.g. SSE intrinsics for ARM NEON), using C/C++ headers
vteProcessor technologiesModels
Abstract machine
Stored-program computer
Finite-state machine
with datapath
Hierarchical
Deterministic finite automaton
Queue automaton
Cellular automaton
Quantum cellular automaton
Turing machine
Alternating Turing machine
Universal
Post–Turing
Quantum
Nondeterministic Turing machine
Probabilistic Turing machine
Hypercomputation
Zeno machine
Belt machine
Stack machine
Register machines
Counter
Pointer
Random-access
Random-access stored program
Architecture
Microarchitecture
Von Neumann
Harvard
modified
Dataflow
Transport-triggered
Cellular
Endianness
Memory access
NUMA
HUMA
Load–store
Register/memory
Cache hierarchy
Memory hierarchy
Virtual memory
Secondary storage
Heterogeneous
Fabric
Multiprocessing
Cognitive
Neuromorphic
Instruction setarchitecturesTypes
Orthogonal instruction set
CISC
RISC
Application-specific
EDGE
TRIPS
VLIW
EPIC
MISC
OISC
NISC
ZISC
VISC architecture
Quantum computing
Comparison
Addressing modes
Instructionsets
Motorola 68000 series
VAX
PDP-11
x86
ARM
Stanford MIPS
MIPS
MIPS-X
Power
POWER
PowerPC
Power ISA
Clipper architecture
SPARC
SuperH
DEC Alpha
ETRAX CRIS
M32R
Unicore
Itanium
OpenRISC
RISC-V
MicroBlaze
LMC
System/3x0
S/360
S/370
S/390
z/Architecture
Tilera ISA
VISC architecture
Epiphany architecture
Others
ExecutionInstruction pipelining
Pipeline stall
Operand forwarding
Classic RISC pipeline
Hazards
Data dependency
Structural
Control
False sharing
Out-of-order
Scoreboarding
Tomasulo's algorithm
Reservation station
Re-order buffer
Register renaming
Wide-issue
Speculative
Branch prediction
Memory dependence prediction
ParallelismLevel
Bit
Bit-serial
Word
Instruction
Pipelining
Scalar
Superscalar
Task
Thread
Process
Data
Vector
Memory
Distributed
Multithreading
Temporal
Simultaneous
Hyperthreading
Simultaneous and heterogenous
Speculative
Preemptive
Cooperative
Flynn's taxonomy
SISD
SIMD
Array processing (SIMT)
Pipelined processing
Associative processing
SWAR
MISD
MIMD
SPMD
Processorperformance
Transistor count
Instructions per cycle (IPC)
Cycles per instruction (CPI)
Instructions per second (IPS)
Floating-point operations per second (FLOPS)
Transactions per second (TPS)
Synaptic updates per second (SUPS)
Performance per watt (PPW)
Cache performance metrics
Computer performance by orders of magnitude
Types
Central processing unit (CPU)
Graphics processing unit (GPU)
GPGPU
Vector
Barrel
Stream
Tile processor
Coprocessor
PAL
ASIC
FPGA
FPOA
CPLD
Multi-chip module (MCM)
System in a package (SiP)
Package on a package (PoP)
By application
Embedded system
Microprocessor
Microcontroller
Mobile
Ultra-low-voltage
ASIP
Soft microprocessor
Systemson chip
System on a chip (SoC)
Multiprocessor (MPSoC)
Cypress PSoC
Network on a chip (NoC)
Hardwareaccelerators
Coprocessor
AI accelerator
Graphics processing unit (GPU)
Image processor
Vision processing unit (VPU)
Physics processing unit (PPU)
Digital signal processor (DSP)
Tensor Processing Unit (TPU)
Secure cryptoprocessor
Network processor
Baseband processor

Word size
1-bit
4-bit
8-bit
12-bit
15-bit
16-bit
24-bit
32-bit
48-bit
64-bit
128-bit
256-bit
512-bit
bit slicing
others
variable
Core count
Single-core
Multi-core
Manycore
Heterogeneous architecture
Components
Core
Cache
CPU cache
Scratchpad memory
Data cache
Instruction cache
replacement policies
coherence
Bus
Clock rate
Clock signal
FIFO
Functionalunits
Arithmetic logic unit (ALU)
Address generation unit (AGU)
Floating-point unit (FPU)
Memory management unit (MMU)
Load–store unit
Translation lookaside buffer (TLB)
Branch predictor
Branch target predictor
Integrated memory controller (IMC)
Memory management unit
Instruction decoder
Logic
Combinational
Sequential
Glue
Logic gate
Quantum
Array
Registers
Processor register
Status register
Stack register
Register file
Memory buffer
Memory address register
Program counter
Control unit
Hardwired control unit
Instruction unit
Data buffer
Write buffer
Microcode ROM
Counter
Datapath
Multiplexer
Demultiplexer
Adder
Multiplier
CPU
Binary decoder
Address decoder
Sum-addressed decoder
Barrel shifter
Circuitry
Integrated circuit
3D
Mixed-signal
Power management
Boolean
Digital
Analog
Quantum
Switch

Powermanagement
PMU
APM
ACPI
Dynamic frequency scaling
Dynamic voltage scaling
Clock gating
Performance per watt (PPW)
Related
History of general-purpose CPUs
Microprocessor chronology
Processor design
Digital electronics
Hardware security module
Semiconductor device fabrication
Tick–tock model
Pin grid array
Chip carrier

vteParallel computingGeneral
Distributed computing
Parallel computing
Massively parallel
Cloud computing
High-performance computing
Multiprocessing
Manycore processor
GPGPU
Computer network
Systolic array
Levels
Bit
Instruction
Thread
Task
Data
Memory
Loop
Pipeline
Multithreading
Temporal
Simultaneous (SMT)
Simultaneous and heterogenous
Speculative (SpMT)
Preemptive
Cooperative
Clustered multi-thread (CMT)
Hardware scout
Theory
PRAM model
PEM model
Analysis of parallel algorithms
Amdahl's law
Gustafson's law
Cost efficiency
Karp–Flatt metric
Slowdown
Speedup
Elements
Process
Thread
Fiber
Instruction window
Array
Coordination
Multiprocessing
Memory coherence
Cache coherence
Cache invalidation
Barrier
Synchronization
Application checkpointing
Programming
Stream processing
Dataflow programming
Models
Implicit parallelism
Explicit parallelism
Concurrency
Non-blocking algorithm
Hardware
Flynn's taxonomy
SISD
SIMD
Array processing (SIMT)
Pipelined processing
Associative processing
MISD
MIMD
Dataflow architecture
Pipelined processor
Superscalar processor
Vector processor
Multiprocessor
symmetric
asymmetric
Memory
shared
distributed
distributed shared
UMA
NUMA
COMA
Massively parallel computer
Computer cluster
Beowulf cluster
Grid computer
Hardware acceleration
APIs
Ateji PX
Boost
Chapel
HPX
Charm++
Cilk
Coarray Fortran
CUDA
Dryad
C++ AMP
Global Arrays
GPUOpen
MPI
OpenMP
OpenCL
OpenHMPP
OpenACC
Parallel Extensions
PVM
pthreads
RaftLib
ROCm
UPC
TBB
ZPL
Problems
Automatic parallelization
Deadlock
Deterministic algorithm
Embarrassingly parallel
Parallel slowdown
Race condition
Software lockout
Scalability
Starvation

 Category: Parallel computing

Authority control databases: National United StatesIsrael




Retrieved from "https://en.wikipedia.org/w/index.php?title=Single_instruction,_multiple_data&oldid=1287294728"
Categories: Classes of computersDigital signal processingFlynn's taxonomyParallel computingSIMD computingHidden categories: Articles with short descriptionShort description is different from WikidataArticles with obsolete information from March 2017All Wikipedia articles in need of updatingAll articles with unsourced statementsArticles with unsourced statements from July 2024Wikipedia articles needing clarification from June 2021Articles with unsourced statements from June 2021Articles with unsourced statements from March 2023Articles with unsourced statements from March 2025Webarchive template wayback links






 This page was last edited on 25 April 2025, at 08:12 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike 4.0 License;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Code of Conduct
Developers
Statistics
Cookie statement
Mobile view














Search













Search









Toggle the table of contents







Single instruction, multiple data




























25 languages


Add topic
















