https://en.wikipedia.org/wiki/Arithmetic_logic_unit




Arithmetic logic unit - Wikipedia



































Jump to content







Main menu





Main menu
move to sidebar
hide



		Navigation
	


Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us





		Contribute
	


HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages



















Search











Search






















Appearance
















Donate

Create account

Log in








Personal tools





Donate Create account Log in





		Pages for logged out editors learn more



ContributionsTalk




























Contents
move to sidebar
hide




(Top)





1
Signals




Toggle Signals subsection





1.1
Data








1.2
Opcode








1.3
Status






1.3.1
Outputs








1.3.2
Inputs












2
Circuit operation








3
Functions




Toggle Functions subsection





3.1
Arithmetic operations








3.2
Bitwise logical operations








3.3
Bit shift operations








3.4
Other operations










4
Applications




Toggle Applications subsection





4.1
Status usage








4.2
Operand and result data paths








4.3
Multiple-precision arithmetic








4.4
Binary fixed-point addition and subtraction








4.5
Complex operations








4.6
Graphics processing units










5
Implementation








6
History








7
See also








8
References








9
Further reading








10
External links


















Toggle the table of contents







Arithmetic logic unit



50 languages




العربيةবাংলাБеларускаяБългарскиCatalàČeštinaDeutschEestiΕλληνικάEspañolEsperantoEuskaraفارسیFrançaisGalego한국어ՀայերենHrvatskiBahasa IndonesiaItalianoעבריתಕನ್ನಡLatviešuLëtzebuergeschLietuviųLombardMagyarമലയാളംBahasa MelayuNederlands日本語Norsk bokmålਪੰਜਾਬੀPolskiPortuguêsRomânăРусскийShqipSimple EnglishSlovenčinaСрпски / srpskiSrpskohrvatski / српскохрватскиSuomiSvenskaTagalogไทยTürkçeУкраїнськаTiếng Việt中文

Edit links











ArticleTalk





English

















ReadEditView history







Tools





Tools
move to sidebar
hide



		Actions
	


ReadEditView history





		General
	


What links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code





		Print/export
	


Download as PDFPrintable version





		In other projects
	


Wikimedia CommonsWikidata item





















Appearance
move to sidebar
hide










From Wikipedia, the free encyclopedia


Combinational digital circuit
A symbolic representation of an ALU and its input and output signals, indicated by arrows pointing into or out of the ALU, respectively. Each arrow represents one or more signals. Control signals enter from the left and status signals exit on the right; data flows from top to bottom.
Part of a series onArithmetic logic circuits
Quick navigation

Theory
 Binary number
 Boolean algebra
 Logic gate
 Ones' complement number
 Two's complement number
 Signed number representations




Components
 

Adder (+)
 Adder
 Half adder
 Full adder
  Ripple-carry adder
 Carry-lookahead adder
 Brent–Kung adder
 Kogge–Stone adder
 Ling adder
 Carry-save adder
 Carry-select adder
 Carry-skip adder




Adder–subtractor (±)
 Adder–subtractor




Subtractor (−)
 Subtractor
 Full subtractor
 Half subtractor




Multiplier (×)
 Binary multiplier
 Multiplication algorithm
 Booth's multiplication algorithm
 Wallace tree
 Dadda multiplier




Divider (÷)
 Binary Divider
 Division algorithm




Bitwise ops
 Bitwise operation
 NOT
 AND
 OR
 XOR
 Bit shifts
 Bit manipulation




See also
 Kochanski multiplication (exponentiation)
  Multiply–accumulate operation







Categories
 Category:Binary arithmetic
 Category:Computer arithmetic




See also
  FPU
  GPU
  AGU
 Mechanical calculator


vte
In computing, an arithmetic logic unit (ALU) is a combinational digital circuit that performs arithmetic and bitwise operations on integer binary numbers.[1][2] This is in contrast to a floating-point unit (FPU), which operates on floating point numbers. It is a fundamental building block of many types of computing circuits, including the central processing unit (CPU) of computers, FPUs, and graphics processing units (GPUs).[3]
The inputs to an ALU are the data to be operated on, called operands, and a code indicating the operation to be performed (opcode); the ALU's output is the result of the performed operation. In many designs, the ALU also has status inputs or outputs, or both, which convey information about a previous operation or the current operation, respectively, between the ALU and external status registers.


Signals[edit]
An ALU has a variety of input and output nets, which are the electrical conductors used to convey digital signals between the ALU and external circuitry. When an ALU is operating, external circuits apply signals to the ALU inputs and, in response, the ALU produces and conveys signals to external circuitry via its outputs.

Data[edit]
A basic ALU has three parallel data buses consisting of two input operands (A and B) and a result output (Y). Each data bus is a group of signals that conveys one binary integer number. Typically, the A, B and Y bus widths (the number of signals comprising each bus) are identical and match the native word size of the external circuitry (e.g., the encapsulating CPU or other processor).

Opcode[edit]
The opcode input is a parallel bus that conveys to the ALU an operation selection code, which is an enumerated value that specifies the desired arithmetic or logic operation to be performed by the ALU. The opcode size (its bus width) determines the maximum number of distinct operations the ALU can perform; for example, a four-bit opcode can specify up to sixteen different ALU operations. Generally, an ALU opcode is not the same as a machine language instruction, though in some cases it may be directly encoded as a bit field within such instructions.

Status[edit]
Outputs[edit]
The status outputs are various individual signals that convey supplemental information about the result of the current ALU operation. General-purpose ALUs commonly have status signals such as:

Carry-out, which conveys the carry resulting from an addition operation, the borrow resulting from a subtraction operation, or the overflow bit resulting from a binary shift operation.
Zero, which indicates all bits of Y are logic zero.
Negative, which indicates the result of an arithmetic operation is negative.
Overflow, which indicates the result of an arithmetic operation has exceeded the numeric range of Y.
Parity, which indicates whether an even or odd number of bits in Y are logic one.
Inputs[edit]
The status inputs allow additional information to be made available to the ALU when performing an operation. Typically, this is a single "carry-in" bit that is the stored carry-out from a previous ALU operation.

Circuit operation[edit]
The combinational logic circuitry of the 74181 integrated circuit, an early four-bit ALU, with logic gates
An ALU is a combinational logic circuit, meaning that its outputs will change asynchronously in response to input changes. In normal operation, stable signals are applied to all of the ALU inputs and, when enough time (known as the "propagation delay") has passed for the signals to propagate through the ALU circuitry, the result of the ALU operation appears at the ALU outputs. The external circuitry connected to the ALU is responsible for ensuring the stability of ALU input signals throughout the operation, and for allowing sufficient time for the signals to propagate through the ALU circuitry before sampling the ALU outputs.
In general, external circuitry controls an ALU by applying signals to the ALU inputs. Typically, the external circuitry employs sequential logic to generate the signals that control ALU operation. The external sequential logic is paced by a clock signal of sufficiently low frequency to ensure enough time for the ALU outputs to settle under worst-case conditions (i.e., conditions resulting in the maximum possible propagation delay).
For example, a CPU starts an addition operation by routing the operands from their sources (typically processor registers) to the ALU's operand inputs, while simultaneously applying a value to the ALU's opcode input that configures it to perform an addition operation. At the same time, the CPU enables the destination register to store the ALU output (the resulting sum from the addition operation) upon operation completion. The ALU's input signals, which are held stable until the next clock, are allowed to propagate through the ALU and to the destination register while the CPU waits for the next clock. When the next clock arrives, the destination register stores the ALU result and, since the ALU operation has completed, the ALU inputs may be set up for the next ALU operation.

Functions[edit]
A number of basic arithmetic and bitwise logic functions are commonly supported by ALUs. Basic, general purpose ALUs typically include these operations in their repertoires:[1][2][4]

Arithmetic operations[edit]
Add: A and B are summed and the sum appears at Y and carry-out.
Add with carry: A, B and carry-in are summed and the sum appears at Y and carry-out.
Subtract: B is subtracted from A (or vice versa) and the difference appears at Y and carry-out. For this function, carry-out is effectively a "borrow" indicator. This operation may also be used to compare the magnitudes of A and B; in such cases the Y output may be ignored by the processor, which is only interested in the status bits (particularly zero and negative) that result from the operation.
Subtract with borrow: B is subtracted from A (or vice versa) with borrow (carry-in) and the difference appears at Y and carry-out (borrow out).
Two's complement: A (or B) is subtracted from zero and the difference appears at Y.
Increment: A (or B) is increased by one and the resulting value appears at Y.
Decrement: A (or B) is decreased by one and the resulting value appears at Y.
Bitwise logical operations[edit]
AND: the bitwise AND of A and B appears at Y.
OR: the bitwise OR of A and B appears at Y.
Exclusive-OR: the bitwise XOR of A and B appears at Y.
Ones' complement: all bits of A (or B) are inverted and appear at Y.
Bit shift operations[edit]

Bit shift examples for an eight-bit ALU


Type

Left

Right


Arithmetic shift






Logical shift






Rotate






Rotate through carry





ALU shift operations cause operand A (or B) to shift left or right (depending on the opcode) and the shifted operand appears at Y. Simple ALUs typically can shift the operand by only one bit position, whereas more complex ALUs employ barrel shifters that allow them to shift the operand by an arbitrary number of bits in one operation. In all single-bit shift operations, the bit shifted out of the operand appears on carry-out; the value of the bit shifted into the operand depends on the type of shift.

Arithmetic shift: the operand is treated as a two's complement integer, meaning that the most significant bit is a "sign" bit and is preserved.
Logical shift: a logic zero is shifted into the operand. This is used to shift unsigned integers.
Rotate: the operand is treated as a circular buffer of bits in which its least and most significant bits are effectively adjacent.
Rotate through carry: the carry bit and operand are collectively treated as a circular buffer of bits.
Other operations[edit]
Pass through: all bits of A (or B) appear unmodified at Y. This operation is typically used to determine the parity of the operand or whether it is zero or negative, or to copy the operand to a processor register.

Applications[edit]
Status usage[edit]
An arithmetic logic unit and its associated status register. The stored carry-out is connected to carry-in to facilitate efficient carry propagation.
Upon completion of each ALU operation, the ALU's status output signals are usually stored in external registers to make them available for future ALU operations (e.g., to implement multiple-precision arithmetic) and for controlling conditional branching. The bit registers that store the status output signals are often collectively treated as a single, multi-bit register, which is referred to as the "status register" or "condition code register".
Depending on the ALU operation being performed, some status register bits may be changed and others may be left unmodified. For example, in bitwise logical operations such as AND and OR, the carry status bit is typically not modified as it is not relevant to such operations.
In CPUs, the stored carry-out signal is usually connected to the ALU's carry-in net. This facilitates efficient propagation of carries (which may represent addition carries, subtraction borrows, or shift overflows) when performing multiple-precision operations, as it eliminates the need for software-management of carry propagation (via conditional branching, based on the carry status bit).

Operand and result data paths[edit]
Block diagram of an example CPU showing data paths for ALU operand sources and result destinations. ALU operands can come from memory, from registers in the register file, or from the instruction being executed. The ALU result can be stored in memory or a processor register.
The sources of ALU operands and destinations of ALU results depend on the architecture of the encapsulating processor and the operation being performed. Processor architectures vary widely, but in general-purpose CPUs, the ALU typically operates in conjunction with a register file (array of processor registers) or accumulator register, which the ALU frequently uses as both a source of operands and a destination for results. To accommodate other operand sources, multiplexers are commonly used to select either the register file or alternative ALU operand sources as required by each machine instruction.
For example, the architecture shown to the right employs a register file with two read ports, which allows the values stored in any two registers (or the same register) to be ALU operands. Alternatively, it allows either ALU operand to be sourced from an immediate operand (a constant value which is directly encoded in the machine instruction[5]) or from memory. The ALU result may be written to any register in the register file or to memory.

Multiple-precision arithmetic[edit]
In integer arithmetic computations, multiple-precision arithmetic is an algorithm that operates on integers which are larger than the ALU word size. To do this, the algorithm treats each integer as an ordered collection of ALU-size fragments, arranged from most-significant (MS) to least-significant (LS) or vice versa. For example, in the case of an 8-bit ALU, the 24-bit integer 0x123456 would be treated as a collection of three 8-bit fragments: 0x12 (MS), 0x34, and 0x56 (LS). Since the size of a fragment exactly matches the ALU word size, the ALU can directly operate on this "piece" of operand.
The algorithm uses the ALU to directly operate on particular operand fragments and thus generate a corresponding fragment (a "partial") of the multi-precision result. Each partial, when generated, is written to an associated region of storage that has been designated for the multiple-precision result. This process is repeated for all operand fragments so as to generate a complete collection of partials, which is the result of the multiple-precision operation.
In arithmetic operations (e.g., addition, subtraction), the algorithm starts by invoking an ALU operation on the operands' LS fragments, thereby producing both a LS partial and a carry out bit. The algorithm writes the partial to designated storage, whereas the processor's state machine typically stores the carry out bit to an ALU status register. The algorithm then advances to the next fragment of each operand's collection and invokes an ALU operation on these fragments along with the stored carry bit from the previous ALU operation, thus producing another (more significant) partial and a carry out bit. As before, the carry bit is stored to the status register and the partial is written to designated storage. This process repeats until all operand fragments have been processed, resulting in a complete collection of partials in storage, which comprise the multi-precision arithmetic result.
In multiple-precision shift operations, the order of operand fragment processing depends on the shift direction. In left-shift operations, fragments are processed LS first because the LS bit of each partial—which is conveyed via the stored carry bit—must be obtained from the MS bit of the previously left-shifted, less-significant operand. Conversely, operands are processed MS first in right-shift operations because the MS bit of each partial must be obtained from the LS bit of the previously right-shifted, more-significant operand.
In bitwise logical operations (e.g., logical AND, logical OR), the operand fragments may be processed in any arbitrary order because each partial depends only on the corresponding operand fragments (the stored carry bit from the previous ALU operation is ignored).

Binary fixed-point addition and subtraction[edit]
Binary fixed-point values are represented by integers. Consequently, for any particular fixed-point scale factor (or implied radix point position), an ALU can directly add or subtract two fixed-point operands and produce a fixed-point result. This capability is commonly used in both fixed-point and floating-point addition and subtraction.
In floating-point addition and subtraction, the significand of the smaller operand is right-shifted so that its fixed-point scale factor matches that of the larger operand. The ALU then adds or subtracts the aligned significands to produce a result significand. Together with other operand elements, the result significand is normalized and rounded to produce the floating-point result.  

Complex operations[edit]
Although it is possible to design ALUs that can perform complex functions, this is usually impractical due to the resulting increases in circuit complexity, power consumption, propagation delay, cost and size. Consequently, ALUs are typically limited to simple functions that can be executed at very high speeds (i.e., very short propagation delays), with more complex functions being the responsibility of software or external circuitry. For example:

In simple cases in which a CPU contains a single ALU, the CPU typically implements a complex operation by orchestrating a sequence of ALU operations according to a software algorithm.
More specialized architectures may use multiple ALUs to accelerate complex operations. In such systems, the ALUs are often pipelined, with intermediate results passing through ALUs arranged like a factory production line. Performance is greatly improved over that of a single ALU because all of the ALUs operate concurrently and software overhead is significantly reduced.
Graphics processing units[edit]
Graphics processing units (GPUs) often contain hundreds or thousands of ALUs which can operate concurrently. Depending on the application and GPU architecture, the ALUs may be used to simultaneously process unrelated data or to operate in parallel on related data. An example of the latter is graphics rendering, in which multiple ALUs perform the same operation in parallel on a group of pixels, with each ALU operating on a pixel within a scene.[6]

Implementation[edit]
An ALU is usually implemented either as a stand-alone integrated circuit (IC), such as the 74181, or as part of a more complex IC. In the latter case, an ALU is typically instantiated by synthesizing it from a description written in VHDL, Verilog or some other hardware description language. For example, the following VHDL code describes a very simple 8-bit ALU:

entity alu is
port (  -- the alu connections to external circuitry:
  A  : in signed(7 downto 0);   -- operand A
  B  : in signed(7 downto 0);   -- operand B
  OP : in unsigned(2 downto 0); -- opcode
  Y  : out signed(7 downto 0));  -- operation result
end alu;

architecture behavioral of alu is
begin
 case OP is  -- decode the opcode and perform the operation:
 when "000" =>  Y <= A + B;   -- add
 when "001" =>  Y <= A - B;   -- subtract
 when "010" =>  Y <= A - 1;   -- decrement
 when "011" =>  Y <= A + 1;   -- increment
 when "100" =>  Y <= not A;   -- 1's complement
 when "101" =>  Y <= A and B; -- bitwise AND
 when "110" =>  Y <= A or B;  -- bitwise OR
 when "111" =>  Y <= A xor B; -- bitwise XOR
 when others => Y <= (others => 'X');
 end case;
end behavioral;

History[edit]
Mathematician John von Neumann proposed the ALU concept in 1945 in a report on the foundations for a new computer called the EDVAC.[7]
The cost, size, and power consumption of electronic circuitry was relatively high throughout the infancy of the Information Age. Consequently, all early computers had a serial ALU that operated on one data bit at a time although they often presented a wider word size to programmers. The first computer to have multiple parallel discrete single-bit ALU circuits was the 1951 Whirlwind I, which employed sixteen such "math units" to enable it to operate on 16-bit words.
In 1967, Fairchild introduced the first ALU-like device implemented as an integrated circuit, the Fairchild 3800, consisting of an eight-bit arithmetic unit with accumulator. It only supported adds and subtracts but no logic functions.[8]
Full integrated-circuit ALUs soon emerged, including four-bit ALUs such as the Am2901 and 74181. These devices were typically "bit slice" capable, meaning they had "carry look ahead" signals that facilitated the use of multiple interconnected ALU chips to create an ALU with a wider word size. These devices quickly became popular and were widely used in bit-slice minicomputers.
Microprocessors began to appear in the early 1970s. Even though transistors had become smaller, there was sometimes insufficient die space for a full-word-width ALU and, as a result, some early microprocessors employed a narrow ALU that required multiple cycles per machine language instruction. Examples of this includes the popular Zilog Z80, which performed eight-bit additions with a four-bit ALU.[9] Over time, transistor geometries shrank further, following Moore's law, and it became feasible to build wider ALUs on microprocessors.
Modern integrated circuit (IC) transistors are orders of magnitude smaller than those of the early microprocessors, making it possible to fit highly complex ALUs on ICs. Today, many modern ALUs have wide word widths, and architectural enhancements such as barrel shifters and binary multipliers[citation needed] that allow them to perform, in a single clock cycle, operations that would have required multiple operations on earlier ALUs.
ALUs can be realized as mechanical, electro-mechanical or electronic circuits[10][failed verification] and, in recent years, research into biological ALUs has been carried out[11][12] (e.g., actin-based).[13]

See also[edit]
Adder (electronics)
Address generation unit (AGU)
Binary multiplier
Execution unit
Load–store unit
Status register
References[edit]


^ a b Atul P. Godse; Deepali A. Godse (2009). "3". Digital Logic Design. Technical Publications. pp. 9–3. ISBN 978-81-8431-738-1.[permanent dead link]

^ a b Atul P. Godse; Deepali A. Godse (2009). "Appendix". Digital Logic Circuits. Technical Publications. pp. C–1. ISBN 978-81-8431-650-6.[permanent dead link]

^ "1. An Introduction to Computer Architecture - Designing Embedded Hardware, 2nd Edition [Book]". www.oreilly.com. Retrieved 2020-09-03.

^ Horowitz, Paul; Winfield Hill (1989). "14.1.1". The Art of Electronics (2nd ed.). Cambridge University Press. pp. 990–. ISBN 978-0-521-37095-0.

^ Barry, Peter; Crowley, Patrick (2012). Modern Embedded Computing. ISBN 978-0-12-391490-3.

^ Smith, Ryan. "Background: How GPUs Work". AnandTech. Retrieved 14 January 2025.

^ Philip Levis (November 8, 2004). "Jonathan von Neumann and EDVAC" (PDF). cs.berkeley.edu. pp. 1, 3. Archived from the original (PDF) on September 23, 2015. Retrieved January 20, 2015.

^ Sherriff, Ken. "Inside the 74181 ALU chip: die photos and reverse engineering". Ken Shirriff's blog. Retrieved 7 May 2024.

^ 
Ken Shirriff.
"The Z-80 has a 4-bit ALU. Here's how it works."
2013, righto.com

^ Reif, John H. (2009), "Mechanical Computing: The Computational Complexity of Physical Devices", in Meyers, Robert A. (ed.), Encyclopedia of Complexity and Systems Science, New York, NY: Springer, pp. 5466–5482, doi:10.1007/978-0-387-30440-3_325, ISBN 978-0-387-30440-3, retrieved 2020-09-03

^ Lin, Chun-Liang; Kuo, Ting-Yu; Li, Wei-Xian (2018-08-14). "Synthesis of control unit for future biocomputer". Journal of Biological Engineering. 12 (1): 14. doi:10.1186/s13036-018-0109-4. ISSN 1754-1611. PMC 6092829. PMID 30127848.

^ Gerd Hg Moe-Behrens. "The biological microprocessor, or how to build a computer with biological parts".

^ Das, Biplab; Paul, Avijit Kumar; De, Debashis (2019-08-16). "An unconventional Arithmetic Logic Unit design and computing in Actin Quantum Cellular Automata". Microsystem Technologies. 28 (3): 809–822. doi:10.1007/s00542-019-04590-1. ISSN 1432-1858. S2CID 202099203.


Further reading[edit]
Hwang, Enoch (2006). Digital Logic and Microprocessor Design with VHDL. Thomson. ISBN 0-534-46593-5.
Stallings, William (2006). Computer Organization & Architecture: Designing for Performance (7th ed.). Pearson Prentice Hall. ISBN 0-13-185644-8.
External links[edit]



Wikimedia Commons has media related to Arithmetic logic units.

vteProcessor technologiesModels
Abstract machine
Stored-program computer
Finite-state machine
with datapath
Hierarchical
Deterministic finite automaton
Queue automaton
Cellular automaton
Quantum cellular automaton
Turing machine
Alternating Turing machine
Universal
Post–Turing
Quantum
Nondeterministic Turing machine
Probabilistic Turing machine
Hypercomputation
Zeno machine
Belt machine
Stack machine
Register machines
Counter
Pointer
Random-access
Random-access stored program
Architecture
Microarchitecture
Von Neumann
Harvard
modified
Dataflow
Transport-triggered
Cellular
Endianness
Memory access
NUMA
HUMA
Load–store
Register/memory
Cache hierarchy
Memory hierarchy
Virtual memory
Secondary storage
Heterogeneous
Fabric
Multiprocessing
Cognitive
Neuromorphic
Instruction setarchitecturesTypes
Orthogonal instruction set
CISC
RISC
Application-specific
EDGE
TRIPS
VLIW
EPIC
MISC
OISC
NISC
ZISC
VISC architecture
Quantum computing
Comparison
Addressing modes
Instructionsets
Motorola 68000 series
VAX
PDP-11
x86
ARM
Stanford MIPS
MIPS
MIPS-X
Power
POWER
PowerPC
Power ISA
Clipper architecture
SPARC
SuperH
DEC Alpha
ETRAX CRIS
M32R
Unicore
Itanium
OpenRISC
RISC-V
MicroBlaze
LMC
System/3x0
S/360
S/370
S/390
z/Architecture
Tilera ISA
VISC architecture
Epiphany architecture
Others
ExecutionInstruction pipelining
Pipeline stall
Operand forwarding
Classic RISC pipeline
Hazards
Data dependency
Structural
Control
False sharing
Out-of-order
Scoreboarding
Tomasulo's algorithm
Reservation station
Re-order buffer
Register renaming
Wide-issue
Speculative
Branch prediction
Memory dependence prediction
ParallelismLevel
Bit
Bit-serial
Word
Instruction
Pipelining
Scalar
Superscalar
Task
Thread
Process
Data
Vector
Memory
Distributed
Multithreading
Temporal
Simultaneous
Hyperthreading
Simultaneous and heterogenous
Speculative
Preemptive
Cooperative
Flynn's taxonomy
SISD
SIMD
Array processing (SIMT)
Pipelined processing
Associative processing
SWAR
MISD
MIMD
SPMD
Processorperformance
Transistor count
Instructions per cycle (IPC)
Cycles per instruction (CPI)
Instructions per second (IPS)
Floating-point operations per second (FLOPS)
Transactions per second (TPS)
Synaptic updates per second (SUPS)
Performance per watt (PPW)
Cache performance metrics
Computer performance by orders of magnitude
Types
Central processing unit (CPU)
Graphics processing unit (GPU)
GPGPU
Vector
Barrel
Stream
Tile processor
Coprocessor
PAL
ASIC
FPGA
FPOA
CPLD
Multi-chip module (MCM)
System in a package (SiP)
Package on a package (PoP)
By application
Embedded system
Microprocessor
Microcontroller
Mobile
Ultra-low-voltage
ASIP
Soft microprocessor
Systemson chip
System on a chip (SoC)
Multiprocessor (MPSoC)
Cypress PSoC
Network on a chip (NoC)
Hardwareaccelerators
Coprocessor
AI accelerator
Graphics processing unit (GPU)
Image processor
Vision processing unit (VPU)
Physics processing unit (PPU)
Digital signal processor (DSP)
Tensor Processing Unit (TPU)
Secure cryptoprocessor
Network processor
Baseband processor

Word size
1-bit
4-bit
8-bit
12-bit
15-bit
16-bit
24-bit
32-bit
48-bit
64-bit
128-bit
256-bit
512-bit
bit slicing
others
variable
Core count
Single-core
Multi-core
Manycore
Heterogeneous architecture
Components
Core
Cache
CPU cache
Scratchpad memory
Data cache
Instruction cache
replacement policies
coherence
Bus
Clock rate
Clock signal
FIFO
Functionalunits
Arithmetic logic unit (ALU)
Address generation unit (AGU)
Floating-point unit (FPU)
Memory management unit (MMU)
Load–store unit
Translation lookaside buffer (TLB)
Branch predictor
Branch target predictor
Integrated memory controller (IMC)
Memory management unit
Instruction decoder
Logic
Combinational
Sequential
Glue
Logic gate
Quantum
Array
Registers
Processor register
Status register
Stack register
Register file
Memory buffer
Memory address register
Program counter
Control unit
Hardwired control unit
Instruction unit
Data buffer
Write buffer
Microcode ROM
Counter
Datapath
Multiplexer
Demultiplexer
Adder
Multiplier
CPU
Binary decoder
Address decoder
Sum-addressed decoder
Barrel shifter
Circuitry
Integrated circuit
3D
Mixed-signal
Power management
Boolean
Digital
Analog
Quantum
Switch

Powermanagement
PMU
APM
ACPI
Dynamic frequency scaling
Dynamic voltage scaling
Clock gating
Performance per watt (PPW)
Related
History of general-purpose CPUs
Microprocessor chronology
Processor design
Digital electronics
Hardware security module
Semiconductor device fabrication
Tick–tock model
Pin grid array
Chip carrier

vteBasic computer componentsInput devicesPointing devices
Graphics tablet
Game controller
Light pen
Mouse
Optical
Optical trackpad
Pointing stick
Touchpad
Touchscreen
Trackball
Other
Keyboard
Image scanner
Graphics card
GPU
Microphone
Refreshable braille display
Sound card
Sound chip
Webcam
Softcam
Output devices
Monitor
Screen
Refreshable braille display
Printer
Plotter
Speakers
Sound card
Graphics card
Removable  data storage
Disk pack
Floppy disk
Optical disc
CD
DVD
Blu-ray
Flash memory
Memory card
USB flash drive
Computer case
Central processing unit
Microprocessor
Motherboard
Memory
RAM
BIOS
Data storage
HDD
SSD (SATA / NVMe)
SSHD
Power supply
SMPS
MOSFET
Power MOSFET
VRM
Network interface controller
Fax modem
Expansion card
PortsCurrent
Ethernet
USB
Thunderbolt
Analog audio jack
DisplayPort
HDMI
Obsolete
FireWire (IEEE 1394)
Parallel port
Serial port
Game port
PS/2 port
eSATA
DVI
VGA
Related
History of computing hardware
History of computing hardware (1960s–present)
List of pioneers in computer science





Retrieved from "https://en.wikipedia.org/w/index.php?title=Arithmetic_logic_unit&oldid=1286298253"
Categories: Digital circuitsCentral processing unitComputer arithmeticComputer architectureArithmetic logic circuitsHidden categories: All articles with dead external linksArticles with dead external links from September 2023Articles with permanently dead external linksArticles with short descriptionShort description is different from WikidataPages using sidebar with the child parameterAll articles with unsourced statementsArticles with unsourced statements from March 2025All articles with failed verificationArticles with failed verification from September 2020Commons category link is on Wikidata






 This page was last edited on 19 April 2025, at 00:30 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike 4.0 License;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Code of Conduct
Developers
Statistics
Cookie statement
Mobile view














Search













Search









Toggle the table of contents







Arithmetic logic unit




























50 languages


Add topic
















