https://en.wikipedia.org/wiki/Multiple_instruction,_multiple_data




Multiple instruction, multiple data - Wikipedia




































Jump to content







Main menu





Main menu
move to sidebar
hide



		Navigation
	


Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us





		Contribute
	


HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages



















Search











Search






















Appearance
















Donate

Create account

Log in








Personal tools





Donate Create account Log in





		Pages for logged out editors learn more



ContributionsTalk




























Contents
move to sidebar
hide




(Top)





1
Examples








2
Shared memory model




Toggle Shared memory model subsection





2.1
Bus-based








2.2
Hierarchical










3
Distributed memory




Toggle Distributed memory subsection





3.1
Hypercube interconnection network








3.2
Mesh interconnection network










4
See also








5
References


















Toggle the table of contents







Multiple instruction, multiple data



19 languages




العربيةCatalàDeutschEspañolفارسیFrançais한국어ItalianoNederlands日本語Norsk bokmålPolskiPortuguêsRomânăРусскийСрпски / srpskiSuomiУкраїнська中文

Edit links











ArticleTalk





English

















ReadEditView history







Tools





Tools
move to sidebar
hide



		Actions
	


ReadEditView history





		General
	


What links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code





		Print/export
	


Download as PDFPrintable version





		In other projects
	


Wikidata item





















Appearance
move to sidebar
hide










From Wikipedia, the free encyclopedia


Computing technique employed to achieve parallelism
Flynn's taxonomy
Single data stream
SISD
MISD

Multiple data streams
SIMD
MIMD

SIMD subcategories[1]
Array processing (SIMT)
Pipelined processing (packed SIMD)
Associative processing (predicated/masked SIMD)

See also
SPMD
MPMD


In computing, multiple instruction, multiple data (MIMD) is a technique employed to achieve parallelism. Machines using MIMD have a number of processor cores that function asynchronously and independently. At any time, different processors may be executing different instructions on different pieces of data.
MIMD architectures may be used in a number of application areas such as computer-aided design/computer-aided manufacturing, simulation, modeling, and as communication switches. MIMD machines can be of either shared memory or distributed memory categories. These classifications are based on how MIMD processors access memory. Shared memory machines may be of the bus-based, extended, or hierarchical type. Distributed memory machines may have hypercube or mesh interconnection schemes.


Examples[edit]
An example of MIMD system is Intel Xeon Phi, descended from Larrabee microarchitecture.[2] These processors have multiple processing cores (up to 61 as of 2015) that can execute different instructions on different data.
Most parallel computers, as of 2013, are MIMD systems.[3]

Shared memory model[edit]
In shared memory model the processors are all connected to a "globally available" memory, via either software or hardware means. The operating system usually maintains its memory coherence.[4]
From a programmer's point of view, this memory model is better understood than the distributed memory model.  Another advantage is that memory coherence is managed by the operating system and not the written program.  Two known disadvantages are: scalability beyond thirty-two processors is difficult, and the shared memory model is less flexible than the distributed memory model.[4]
There are many examples of shared memory (multiprocessors): UMA (uniform memory access), COMA (cache-only memory access).[5]

Bus-based[edit]
MIMD machines with shared memory have processors which share a common, central memory. In the simplest form, all processors are attached to a bus which connects them to memory. This means that every machine with shared memory shares a specific CM, common bus system for all the clients.
For example, if we consider a bus with clients A, B, C connected on one side and P, Q, R connected on the opposite side,
any one of the clients will communicate with the other by means of the bus interface between them.

Hierarchical[edit]
MIMD machines with hierarchical shared memory use a hierarchy of buses (as, for example, in a "fat tree") to give processors access to each other's memory. Processors on different boards may communicate through inter-nodal buses. Buses support communication between boards. With this type of architecture, the machine may support over nine thousand processors.

Distributed memory[edit]
In distributed memory MIMD (multiple instruction, multiple data) machines, each processor has its own individual memory location. Each processor has no direct knowledge about other processor's memory. For data to be shared, it must be passed from one processor to another as a message. Since there is no shared memory, contention is not as great a problem with these machines. It is not economically feasible to connect a large number of processors directly to each other. A way to avoid this multitude of direct connections is to connect each processor to just a few others. This type of design can be inefficient because of the added time required to pass a message from one processor to another along the message path. The amount of time required for processors to perform simple message routing can be substantial. Systems were designed to reduce this time loss and hypercube and mesh are among two of the popular interconnection schemes.
Examples of distributed memory (multiple computers) include MPP (massively parallel processors), COW (clusters of workstations) and NUMA (non-uniform memory access). The former is complex and expensive: Many super-computers coupled by broad-band networks. Examples include hypercube and mesh interconnections. COW is the "home-made" version for a fraction of the price.[5]

Hypercube interconnection network[edit]
In an MIMD distributed memory machine with a hypercube system interconnection network containing four processors, a processor and a memory module are placed at each vertex of a square. The diameter of the system is the minimum number of steps it takes for one processor to send a message to the processor that is the farthest away. So, for example, the diameter of a 2-cube is 2. In a hypercube system with eight processors and each processor and memory module being placed in the vertex of a cube, the diameter is 3. In general, a system that contains 2^N processors with each processor directly connected to N other processors, the diameter of the system is N. One disadvantage of a hypercube system is that it must be configured in powers of two, so a machine must be built that could potentially have many more processors than is really needed for the application.

Mesh interconnection network[edit]
In an MIMD distributed memory machine with a mesh interconnection network, processors are placed in a two-dimensional grid. Each processor is connected to its four immediate neighbors. Wrap around connections may be provided at the edges of the mesh. One advantage of the mesh interconnection network over the hypercube is that the mesh system need not be configured in powers of two. A disadvantage is that the diameter of the mesh network is greater than the hypercube for systems with more than four processors.

See also[edit]
SMP
NUMA
Torus interconnect
Flynn's taxonomy
SPMD
Superscalar
Very long instruction word
References[edit]


^ Flynn, Michael J. (September 1972). "Some Computer Organizations and Their Effectiveness" (PDF). IEEE Transactions on Computers. C-21 (9): 948–960. doi:10.1109/TC.1972.5009071.

^ "The Perils of Parallel: Larrabee vs. Nvidia, MIMD vs. SIMD". 19 September 2008.

^ "MIMD | Intel® Developer Zone". Archived from the original on 2013-10-16. Retrieved 2013-10-16.

^ a b Ibaroudene, Djaffer. "Parallel Processing, EG6370G: Chapter 1, Motivation and History." Lecture Slides. St Mary's University, San Antonio, Texas. Spring 2008.

^ a b Andrew S. Tanenbaum (1997). Structured Computer Organization (4 ed.). Prentice-Hall. pp. 559–585. ISBN 978-0130959904. Archived from the original on 2013-12-01. Retrieved 2013-03-15.


vteProcessor technologiesModels
Abstract machine
Stored-program computer
Finite-state machine
with datapath
Hierarchical
Deterministic finite automaton
Queue automaton
Cellular automaton
Quantum cellular automaton
Turing machine
Alternating Turing machine
Universal
Post–Turing
Quantum
Nondeterministic Turing machine
Probabilistic Turing machine
Hypercomputation
Zeno machine
Belt machine
Stack machine
Register machines
Counter
Pointer
Random-access
Random-access stored program
Architecture
Microarchitecture
Von Neumann
Harvard
modified
Dataflow
Transport-triggered
Cellular
Endianness
Memory access
NUMA
HUMA
Load–store
Register/memory
Cache hierarchy
Memory hierarchy
Virtual memory
Secondary storage
Heterogeneous
Fabric
Multiprocessing
Cognitive
Neuromorphic
Instruction setarchitecturesTypes
Orthogonal instruction set
CISC
RISC
Application-specific
EDGE
TRIPS
VLIW
EPIC
MISC
OISC
NISC
ZISC
VISC architecture
Quantum computing
Comparison
Addressing modes
Instructionsets
Motorola 68000 series
VAX
PDP-11
x86
ARM
Stanford MIPS
MIPS
MIPS-X
Power
POWER
PowerPC
Power ISA
Clipper architecture
SPARC
SuperH
DEC Alpha
ETRAX CRIS
M32R
Unicore
Itanium
OpenRISC
RISC-V
MicroBlaze
LMC
System/3x0
S/360
S/370
S/390
z/Architecture
Tilera ISA
VISC architecture
Epiphany architecture
Others
ExecutionInstruction pipelining
Pipeline stall
Operand forwarding
Classic RISC pipeline
Hazards
Data dependency
Structural
Control
False sharing
Out-of-order
Scoreboarding
Tomasulo's algorithm
Reservation station
Re-order buffer
Register renaming
Wide-issue
Speculative
Branch prediction
Memory dependence prediction
ParallelismLevel
Bit
Bit-serial
Word
Instruction
Pipelining
Scalar
Superscalar
Task
Thread
Process
Data
Vector
Memory
Distributed
Multithreading
Temporal
Simultaneous
Hyperthreading
Simultaneous and heterogenous
Speculative
Preemptive
Cooperative
Flynn's taxonomy
SISD
SIMD
Array processing (SIMT)
Pipelined processing
Associative processing
SWAR
MISD
MIMD
SPMD
Processorperformance
Transistor count
Instructions per cycle (IPC)
Cycles per instruction (CPI)
Instructions per second (IPS)
Floating-point operations per second (FLOPS)
Transactions per second (TPS)
Synaptic updates per second (SUPS)
Performance per watt (PPW)
Cache performance metrics
Computer performance by orders of magnitude
Types
Central processing unit (CPU)
Graphics processing unit (GPU)
GPGPU
Vector
Barrel
Stream
Tile processor
Coprocessor
PAL
ASIC
FPGA
FPOA
CPLD
Multi-chip module (MCM)
System in a package (SiP)
Package on a package (PoP)
By application
Embedded system
Microprocessor
Microcontroller
Mobile
Ultra-low-voltage
ASIP
Soft microprocessor
Systemson chip
System on a chip (SoC)
Multiprocessor (MPSoC)
Cypress PSoC
Network on a chip (NoC)
Hardwareaccelerators
Coprocessor
AI accelerator
Graphics processing unit (GPU)
Image processor
Vision processing unit (VPU)
Physics processing unit (PPU)
Digital signal processor (DSP)
Tensor Processing Unit (TPU)
Secure cryptoprocessor
Network processor
Baseband processor

Word size
1-bit
4-bit
8-bit
12-bit
15-bit
16-bit
24-bit
32-bit
48-bit
64-bit
128-bit
256-bit
512-bit
bit slicing
others
variable
Core count
Single-core
Multi-core
Manycore
Heterogeneous architecture
Components
Core
Cache
CPU cache
Scratchpad memory
Data cache
Instruction cache
replacement policies
coherence
Bus
Clock rate
Clock signal
FIFO
Functionalunits
Arithmetic logic unit (ALU)
Address generation unit (AGU)
Floating-point unit (FPU)
Memory management unit (MMU)
Load–store unit
Translation lookaside buffer (TLB)
Branch predictor
Branch target predictor
Integrated memory controller (IMC)
Memory management unit
Instruction decoder
Logic
Combinational
Sequential
Glue
Logic gate
Quantum
Array
Registers
Processor register
Status register
Stack register
Register file
Memory buffer
Memory address register
Program counter
Control unit
Hardwired control unit
Instruction unit
Data buffer
Write buffer
Microcode ROM
Counter
Datapath
Multiplexer
Demultiplexer
Adder
Multiplier
CPU
Binary decoder
Address decoder
Sum-addressed decoder
Barrel shifter
Circuitry
Integrated circuit
3D
Mixed-signal
Power management
Boolean
Digital
Analog
Quantum
Switch

Powermanagement
PMU
APM
ACPI
Dynamic frequency scaling
Dynamic voltage scaling
Clock gating
Performance per watt (PPW)
Related
History of general-purpose CPUs
Microprocessor chronology
Processor design
Digital electronics
Hardware security module
Semiconductor device fabrication
Tick–tock model
Pin grid array
Chip carrier

vteParallel computingGeneral
Distributed computing
Parallel computing
Massively parallel
Cloud computing
High-performance computing
Multiprocessing
Manycore processor
GPGPU
Computer network
Systolic array
Levels
Bit
Instruction
Thread
Task
Data
Memory
Loop
Pipeline
Multithreading
Temporal
Simultaneous (SMT)
Simultaneous and heterogenous
Speculative (SpMT)
Preemptive
Cooperative
Clustered multi-thread (CMT)
Hardware scout
Theory
PRAM model
PEM model
Analysis of parallel algorithms
Amdahl's law
Gustafson's law
Cost efficiency
Karp–Flatt metric
Slowdown
Speedup
Elements
Process
Thread
Fiber
Instruction window
Array
Coordination
Multiprocessing
Memory coherence
Cache coherence
Cache invalidation
Barrier
Synchronization
Application checkpointing
Programming
Stream processing
Dataflow programming
Models
Implicit parallelism
Explicit parallelism
Concurrency
Non-blocking algorithm
Hardware
Flynn's taxonomy
SISD
SIMD
Array processing (SIMT)
Pipelined processing
Associative processing
MISD
MIMD
Dataflow architecture
Pipelined processor
Superscalar processor
Vector processor
Multiprocessor
symmetric
asymmetric
Memory
shared
distributed
distributed shared
UMA
NUMA
COMA
Massively parallel computer
Computer cluster
Beowulf cluster
Grid computer
Hardware acceleration
APIs
Ateji PX
Boost
Chapel
HPX
Charm++
Cilk
Coarray Fortran
CUDA
Dryad
C++ AMP
Global Arrays
GPUOpen
MPI
OpenMP
OpenCL
OpenHMPP
OpenACC
Parallel Extensions
PVM
pthreads
RaftLib
ROCm
UPC
TBB
ZPL
Problems
Automatic parallelization
Deadlock
Deterministic algorithm
Embarrassingly parallel
Parallel slowdown
Race condition
Software lockout
Scalability
Starvation

 Category: Parallel computing

Authority control databases: National United StatesIsrael




Retrieved from "https://en.wikipedia.org/w/index.php?title=Multiple_instruction,_multiple_data&oldid=1235681846"
Categories: Flynn's taxonomyParallel computingClasses of computersHidden categories: Articles with short descriptionShort description is different from Wikidata






 This page was last edited on 20 July 2024, at 16:27 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike 4.0 License;
additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Code of Conduct
Developers
Statistics
Cookie statement
Mobile view














Search













Search









Toggle the table of contents







Multiple instruction, multiple data




























19 languages


Add topic
















